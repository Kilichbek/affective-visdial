<h1>Welcome to ICCV23 Visual-Dialog Based Emotion Explanation Generation Challenge!</h1>

<h2>Challenge Overview</h2>
<p>This unique competition invites researchers and enthusiasts to delve into the fascinating realm of dialogue-driven emotion analysis and explanation generation within the context of art discussions.</p>

<h3>Task: Generating Emotion and Explanations</h3>
<p>Participants are tasked with a captivating challenge: given a dialog revolving around art, their goal is to automatically predict and generate both emotional labels and detailed explanations in natural language. This entails comprehending the given conversation and crafting explanations that not only capture the emotional essence but also present it in a coherent and informative manner.</p>

<h3>Why Join?</h3>
<p>This challenge provides an exciting platform for showcasing your expertise in natural language understanding, emotion recognition, and explanation generation. By participating in this challenge, you'll have the opportunity to:</p>
<ul>
    <li>Contribute to cutting-edge research at the intersection of visually grounded dialogue, emotion analysis, and language generation.</li>
    <li>Compete with fellow researchers and practitioners to showcase your algorithm's prowess.</li>
    <li>Gain exposure within the AI and computer vision communities through the prestigious ICCV23 venue.</li>
    <li>Win cash prizes and awards! <span style="color: red">TODO: PRIZE</span></li>
</ul>

<h3>Dataset</h3>
<p>Our curated dataset comprises rich and diverse dialogues centered around art pieces.
This dataset promises to challenge your model's capacity for emotion comprehension, linguistic expression, and explanation coherence.</p>
Each sample in the dataset consists of:
<ul>
    <li>2 initial opinions/captions about the hidden art used to initiate the conversation.</li>
    <li>A conversation itself with 10 question-answer pairs about the image.</li>
    <li>At the end of the conversation, we provide ground truth emotion labels and emotion explanations for training.</li>
</ul>
The desired model should take the conversation (and optionally 2 initial opinions) as input and generate the emotion label and explanation for the image.

<p>Dataset splits are available here:</p>
    is availabe: <a href="#">here</a>
    <ul>
        <li><a href="#">Train Set</a></li>
        <li><a href="#">Val Set</a></li>
        <li><a href="#">Test Set</a></li>
    </ul>

<p>Dataset has the following format:</p>
<pre><code>[
    {
        "dialog_id": "DIALOG_ID_1",
        "emotion_label_1": "EMOTION_LABEL_1",
        "emotion_label_2": "EMOTION_LABEL_2",
        "caption_1": "CAPTION_1",
        "caption_2": "CAPTION_2",
        "dialog": [
            {
                "question": "QUESTION_1",
                "answer": "ANSWER_1"
            },
            {
                "question": "QUESTION_2",
                "answer": "ANSWER_2"
            },
            ...
        ]
        "emotion_label" : "EMOTION_LABEL",
        "emotion_explanation": "EMOTION_EXPLANATION",
    },
    {
        "dialog_id": "DIALOG_ID_1",
        "emotion_label_1": "EMOTION_LABEL_1",
        "emotion_label_2": "EMOTION_LABEL_2",
        "caption_1": "CAPTION_1",
        "caption_2": "CAPTION_2",
        "dialog": [
            {
                "question": "QUESTION_1",
                "answer": "ANSWER_1"
            },
            {
                "question": "QUESTION_2",
                "answer": "ANSWER_2"
            },
            ...
        ]
        "emotion_label" : "EMOTION_LABEL",
        "emotion_explanation": "EMOTION_EXPLANATION",
    },
    ...
    ]
    </code></pre>
<p style="color: red;"><strong>Participants should not use any external data for training purposes other than the data provided by us. Teams found doing so will be disqualified from the challenge.</strong></p>
<p>However, you are free to utilize large pre-trained systems.</p>

<h3>Evaluation Metrics</h3>
<p>Models will be evaluated based on the quality of their predicted emotional labels and the effectiveness of their generated explanations. 
Our evaluation metrics take into account both the accuracy of emotion prediction and the linguistic quality of the explanations, providing a comprehensive assessment of model performance.</p>
The performance will be evaluated against ground truth in terms of the following metrics:
<ul>
    <li>Weighted F1 score</li>
    <li>BLEU score</li>
    <li>BERT score</li>
</ul>

<h3>Important Dates</h3>
<ul>
    <li>Challenge Launch: 25 Aug 2023  <span style="color: red">TODO: time</span></li>
    <li>Submission Deadline: 25 Sep 2023  <span style="color: red">TODO: time</span></li>
    <li>Results Announcement:  02 Oct 2023  <span style="color: red">TODO: time</span></li></li>
    <li>ICCV23 Conference: 02 Oct 2023  <span style="color: red">TODO: time</span></li>
</ul>